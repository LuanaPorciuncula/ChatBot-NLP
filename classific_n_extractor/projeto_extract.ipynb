{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "projeto-extract.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "#http://alexminnaar.com/2019/08/22/ner-rnns-tensorflow.html\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(tf.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.4\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcXfKcxq6h3s",
        "outputId": "cbd8847f-5d86-4da7-e49d-c01da2db0ce1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "import pandas\n",
        "import ast\n",
        "\n",
        "pathTrain = \"trains.csv\"\n",
        "pathTest = \"tests.csv\"\n",
        "pathLabels = \"labels.csv\"\n",
        "\n",
        "train_dataset = pandas.read_csv(pathTrain, index_col=0).applymap(ast.literal_eval).values.tolist()\n",
        "test_dataset = pandas.read_csv(pathTest, index_col=0).applymap(ast.literal_eval).values.tolist()\n",
        "labels = set(pandas.read_csv(pathLabels, index_col=0).values.flatten())\n",
        "\n",
        "print(train_dataset[0:5])\n",
        "print(\"Labels:\", labels)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['i', 'need', 'a', 'place', 'to', 'dine', 'in', 'the', 'center', 'thats', 'expensive'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'restaurant-pricerange']], [['any', 'sort', 'of', 'food', 'would', 'be', 'fine', 'as', 'long', 'as', 'it', 'is', 'a', 'bit', 'expensive', '.', 'could', 'i', 'get', 'the', 'phone', 'number', 'for', 'your', 'recommendation', '?'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'restaurant-pricerange', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']], [['sounds', 'good', 'could', 'i', 'get', 'that', 'phone', 'number', 'also', 'could', 'you', 'recommend', 'me', 'an', 'expensive', 'hotel', '?'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'restaurant-pricerange', 'O', 'O']], [['yeah', 'i', 'need', 'a', 'restaurant', 'in', 'the', 'west', 'and', 'with', 'expensive', 'pricing', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'restaurant-area', 'O', 'O', 'restaurant-pricerange', 'O', 'O']], [['it', 's', 'a', 'place', 'called', 'don pasquale pizzeria', '.'], ['O', 'O', 'O', 'O', 'O', 'restaurant-name', 'O']]]\n",
            "Labels: {'restaurant-bookday', 'restaurant-name', 'restaurant-pricerange', 'restaurant-area', 'restaurant-food', 'O', 'restaurant-booktime', 'restaurant-bookpeople'}\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wYhRwHJ9My8",
        "outputId": "2360ca69-4049-433e-f2c1-e43e28d9353f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# for i, label in enumerate(train_dataset[1]):\n",
        "#     if 'other' in label:\n",
        "#       train_dataset[1][i] = 'O'\n",
        "\n",
        "# for i, label in enumerate(test_dataset[1]):\n",
        "#     if 'other' in label:\n",
        "#       test_dataset[1][i] = 'O'\n",
        "      \n",
        "# if 'other' in labels: labels.remove('other')\n",
        "# labels.add('O')\n",
        "# print(labels)\n",
        "\n",
        "# create character vocab\n",
        "all_text = \" \".join([\" \".join(x[0]) for x in train_dataset+test_dataset])\n",
        "vocab = sorted(set(all_text))\n",
        "\n",
        "# create character/id and label/id mapping\n",
        "char2idx = {u:i+1 for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "label2idx = {u:i+1 for i, u in enumerate(labels)}\n",
        "idx2label = np.array(labels)\n",
        "\n",
        "print(idx2label)\n",
        "print(char2idx)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'restaurant-bookday', 'restaurant-name', 'restaurant-pricerange', 'restaurant-area', 'restaurant-food', 'O', 'restaurant-booktime', 'restaurant-bookpeople'}\n",
            "{' ': 1, '!': 2, '#': 3, '%': 4, '&': 5, \"'\": 6, '*': 7, ',': 8, '-': 9, '.': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, '?': 22, '`': 23, 'a': 24, 'b': 25, 'c': 26, 'd': 27, 'e': 28, 'f': 29, 'g': 30, 'h': 31, 'i': 32, 'j': 33, 'k': 34, 'l': 35, 'm': 36, 'n': 37, 'o': 38, 'p': 39, 'q': 40, 'r': 41, 's': 42, 't': 43, 'u': 44, 'v': 45, 'w': 46, 'x': 47, 'y': 48, 'z': 49}\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ank-S_QRTpYU",
        "outputId": "c9ab7189-8708-4109-d67b-e12f08b36154"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "def split_char_labels(eg):\n",
        "  '''\n",
        "  For a given input/output example, break tokens into characters while keeping \n",
        "  the same label.\n",
        "  '''\n",
        "\n",
        "  tokens = eg[0]\n",
        "  labels = eg[1]\n",
        "\n",
        "  input_chars = []\n",
        "  output_char_labels = []\n",
        "\n",
        "  for token,label in zip(tokens,labels):\n",
        "    input_chars.extend([char for char in token])\n",
        "    input_chars.extend(' ')\n",
        "    output_char_labels.extend([label]*len(token))\n",
        "    output_char_labels.extend('O')\n",
        "\n",
        "  return [[char2idx[x] for x in input_chars[:-1]], np.array([label2idx[x] for x in output_char_labels[:-1]])]\n",
        "\n",
        "\n",
        "train_formatted = [split_char_labels(eg) for eg in train_dataset]\n",
        "test_formatted = [split_char_labels(eg) for eg in test_dataset]\n",
        "\n",
        "print(len(train_formatted))\n",
        "print(len(test_formatted))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10193\n",
            "1263\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU3whO-CXynY",
        "outputId": "9fcb7656-9bad-44fa-e6c5-6a2e6a9cd6f8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# training generator\n",
        "def gen_train_series():\n",
        "\n",
        "    for eg in train_formatted:\n",
        "      yield eg[0],eg[1]\n",
        "\n",
        "# validation generator\n",
        "def gen_valid_series():\n",
        "\n",
        "    for eg in valid_formatted:\n",
        "      yield eg[0],eg[1]\n",
        "\n",
        "# test generator\n",
        "def gen_test_series():\n",
        "\n",
        "  for eg in test_formatted:\n",
        "      yield eg[0],eg[1]\n",
        "  \n",
        "# create Dataset objects for train, test and validation sets  \n",
        "series = tf.data.Dataset.from_generator(gen_train_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
        "series_test = tf.data.Dataset.from_generator(gen_test_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE=1000\n",
        "\n",
        "# create padded batch series objects for train, test and validation sets\n",
        "ds_series_batch = series.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
        "ds_series_batch_test = series_test.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
        "\n",
        "# print example batches\n",
        "for input_example_batch, target_example_batch in ds_series_batch_test.take(1):\n",
        "  print(input_example_batch)\n",
        "  print(target_example_batch)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[31 28 35 ...  0  0  0]\n",
            " [26 24 37 ...  0  0  0]\n",
            " [24 26 43 ...  0  0  0]\n",
            " ...\n",
            " [32  1 46 ...  0  0  0]\n",
            " [26 24 37 ...  0  0  0]\n",
            " [43 41 48 ...  0  0  0]], shape=(128, 167), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[6 6 6 ... 0 0 0]\n",
            " [6 6 6 ... 0 0 0]\n",
            " [6 6 6 ... 0 0 0]\n",
            " ...\n",
            " [6 6 6 ... 0 0 0]\n",
            " [6 6 6 ... 0 0 0]\n",
            " [6 6 6 ... 0 0 0]], shape=(128, 167), dtype=int32)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_JyTYupffwU",
        "outputId": "21d2a2f5-3b41-4618-dcaf-9061142af0ee"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "vocab_size = len(vocab)+1\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 256\n",
        "\n",
        "label_size = len(labels)  \n",
        "\n",
        "# build LSTM model\n",
        "def build_model(vocab_size,label_size, embedding_dim, rnn_units, batch_size):\n",
        "      model = tf.keras.Sequential([\n",
        "          tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                    batch_input_shape=[batch_size, None],mask_zero=True),\n",
        "          tf.keras.layers.LSTM(rnn_units,\n",
        "                               return_sequences=True,\n",
        "                               stateful=True,\n",
        "                               recurrent_initializer='glorot_uniform'),\n",
        "          tf.keras.layers.Dense(label_size)\n",
        "          ])\n",
        "      return model\n",
        "\n",
        "model = build_model(\n",
        "        vocab_size=len(vocab)+1,\n",
        "        label_size=len(labels)+1,\n",
        "        embedding_dim=embedding_dim,\n",
        "        rnn_units=rnn_units,\n",
        "        batch_size=BATCH_SIZE)\n",
        "\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (128, None, 256)          12800     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (128, None, 256)          525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (128, None, 9)            2313      \n",
            "=================================================================\n",
            "Total params: 540,425\n",
            "Trainable params: 540,425\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "id": "KpnnY0sago83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1672800-6192-4e44-bfe2-dfb96d60672d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "import os\n",
        "# !pip install tensorflow-addons\n",
        "# import tensorflow_addons as tfa\n",
        "# import keras.backend as K\n",
        "# from sklearn.metrics import f1_score\n",
        "\n",
        "# def f1_metric(y_true, y_pred):\n",
        "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "#     precision = true_positives / (predicted_positives + K.epsilon())\n",
        "#     recall = true_positives / (possible_positives + K.epsilon())\n",
        "#     f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "#     return f1_val\n",
        "\n",
        "\n",
        "# define loss function\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# def f1Score(labels, logits):\n",
        "#   print(labels.numpy().shape, logits.numpy().shape)\n",
        "#   return f1_score(labels.numpy(), logits.numpy())\n",
        "\n",
        "# def recall(y_true, y_pred):\n",
        "#     y_true = K.ones_like(y_true) \n",
        "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#     all_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    \n",
        "#     recall = true_positives / (all_positives + K.epsilon())\n",
        "#     return recall\n",
        "\n",
        "# def precision(y_true, y_pred):\n",
        "#     y_true = K.ones_like(y_true) \n",
        "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    \n",
        "#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "#     precision = true_positives / (predicted_positives + K.epsilon())\n",
        "#     return precision\n",
        "\n",
        "# def f1_score(y_true, y_pred):\n",
        "#     precision_m = precision(y_true, y_pred)\n",
        "#     recall_m = recall(y_true, y_pred)\n",
        "#     return 2*((precision_m*recall_m)/(precision_m+recall_m+K.epsilon()))\n",
        "\n",
        "# model.compile(optimizer='adam', loss=loss,metrics=[tf.keras.metrics.SparseCategoricalAccuracy(), f1_metric])\n",
        "model.compile(optimizer='adam', loss=loss,metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "cTP-FbDbgyPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e851409-ff8c-49d8-f782-969327ba09b2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "EPOCHS=45 # aparentemente melhor valor é 45, pq os valores não melhoram significativamente e o collab fica bem mais lento depois\n",
        "history = model.fit(ds_series_batch, epochs=EPOCHS, validation_data=ds_series_batch_test,callbacks=[checkpoint_callback])\n",
        "# pra o keras pegar o melhor: checkpoint_callback"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/45\n",
            "79/79 [==============================] - 69s 873ms/step - loss: 0.2902 - sparse_categorical_accuracy: 0.8363 - val_loss: 0.2366 - val_sparse_categorical_accuracy: 0.8497\n",
            "Epoch 2/45\n",
            "79/79 [==============================] - 65s 821ms/step - loss: 0.1890 - sparse_categorical_accuracy: 0.8656 - val_loss: 0.1542 - val_sparse_categorical_accuracy: 0.8815\n",
            "Epoch 3/45\n",
            "79/79 [==============================] - 68s 857ms/step - loss: 0.1307 - sparse_categorical_accuracy: 0.8972 - val_loss: 0.1201 - val_sparse_categorical_accuracy: 0.9067\n",
            "Epoch 4/45\n",
            "79/79 [==============================] - 66s 830ms/step - loss: 0.1004 - sparse_categorical_accuracy: 0.9214 - val_loss: 0.1024 - val_sparse_categorical_accuracy: 0.9189\n",
            "Epoch 5/45\n",
            "79/79 [==============================] - 73s 927ms/step - loss: 0.0838 - sparse_categorical_accuracy: 0.9331 - val_loss: 0.0807 - val_sparse_categorical_accuracy: 0.9366\n",
            "Epoch 6/45\n",
            "79/79 [==============================] - 75s 946ms/step - loss: 0.0724 - sparse_categorical_accuracy: 0.9420 - val_loss: 0.0732 - val_sparse_categorical_accuracy: 0.9420\n",
            "Epoch 7/45\n",
            "79/79 [==============================] - 72s 916ms/step - loss: 0.0651 - sparse_categorical_accuracy: 0.9468 - val_loss: 0.0628 - val_sparse_categorical_accuracy: 0.9486\n",
            "Epoch 8/45\n",
            "79/79 [==============================] - 64s 816ms/step - loss: 0.0584 - sparse_categorical_accuracy: 0.9525 - val_loss: 0.0583 - val_sparse_categorical_accuracy: 0.9517\n",
            "Epoch 9/45\n",
            "79/79 [==============================] - 68s 856ms/step - loss: 0.0544 - sparse_categorical_accuracy: 0.9548 - val_loss: 0.0543 - val_sparse_categorical_accuracy: 0.9544\n",
            "Epoch 10/45\n",
            "79/79 [==============================] - 72s 915ms/step - loss: 0.0502 - sparse_categorical_accuracy: 0.9587 - val_loss: 0.0508 - val_sparse_categorical_accuracy: 0.9577\n",
            "Epoch 11/45\n",
            "79/79 [==============================] - 69s 873ms/step - loss: 0.0461 - sparse_categorical_accuracy: 0.9612 - val_loss: 0.0478 - val_sparse_categorical_accuracy: 0.9592\n",
            "Epoch 12/45\n",
            "79/79 [==============================] - 69s 873ms/step - loss: 0.0430 - sparse_categorical_accuracy: 0.9636 - val_loss: 0.0470 - val_sparse_categorical_accuracy: 0.9605\n",
            "Epoch 13/45\n",
            "79/79 [==============================] - 104s 1s/step - loss: 0.0410 - sparse_categorical_accuracy: 0.9655 - val_loss: 0.0445 - val_sparse_categorical_accuracy: 0.9633\n",
            "Epoch 14/45\n",
            "79/79 [==============================] - 74s 939ms/step - loss: 0.0385 - sparse_categorical_accuracy: 0.9676 - val_loss: 0.0423 - val_sparse_categorical_accuracy: 0.9650\n",
            "Epoch 15/45\n",
            "79/79 [==============================] - 69s 878ms/step - loss: 0.0357 - sparse_categorical_accuracy: 0.9695 - val_loss: 0.0401 - val_sparse_categorical_accuracy: 0.9658\n",
            "Epoch 16/45\n",
            "79/79 [==============================] - 68s 855ms/step - loss: 0.0350 - sparse_categorical_accuracy: 0.9706 - val_loss: 0.0403 - val_sparse_categorical_accuracy: 0.9665\n",
            "Epoch 17/45\n",
            "79/79 [==============================] - 66s 842ms/step - loss: 0.0332 - sparse_categorical_accuracy: 0.9721 - val_loss: 0.0381 - val_sparse_categorical_accuracy: 0.9680\n",
            "Epoch 18/45\n",
            "79/79 [==============================] - 64s 815ms/step - loss: 0.0321 - sparse_categorical_accuracy: 0.9735 - val_loss: 0.0376 - val_sparse_categorical_accuracy: 0.9692\n",
            "Epoch 19/45\n",
            "79/79 [==============================] - 68s 856ms/step - loss: 0.0306 - sparse_categorical_accuracy: 0.9740 - val_loss: 0.0367 - val_sparse_categorical_accuracy: 0.9693\n",
            "Epoch 20/45\n",
            "79/79 [==============================] - 65s 817ms/step - loss: 0.0289 - sparse_categorical_accuracy: 0.9756 - val_loss: 0.0359 - val_sparse_categorical_accuracy: 0.9706\n",
            "Epoch 21/45\n",
            "79/79 [==============================] - 62s 785ms/step - loss: 0.0271 - sparse_categorical_accuracy: 0.9767 - val_loss: 0.0352 - val_sparse_categorical_accuracy: 0.9709\n",
            "Epoch 22/45\n",
            "79/79 [==============================] - 63s 796ms/step - loss: 0.0265 - sparse_categorical_accuracy: 0.9777 - val_loss: 0.0373 - val_sparse_categorical_accuracy: 0.9701\n",
            "Epoch 23/45\n",
            "79/79 [==============================] - 60s 764ms/step - loss: 0.0260 - sparse_categorical_accuracy: 0.9779 - val_loss: 0.0346 - val_sparse_categorical_accuracy: 0.9720\n",
            "Epoch 24/45\n",
            "79/79 [==============================] - 61s 766ms/step - loss: 0.0248 - sparse_categorical_accuracy: 0.9788 - val_loss: 0.0344 - val_sparse_categorical_accuracy: 0.9717\n",
            "Epoch 25/45\n",
            "79/79 [==============================] - 62s 780ms/step - loss: 0.0237 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.0356 - val_sparse_categorical_accuracy: 0.9716\n",
            "Epoch 26/45\n",
            "79/79 [==============================] - 61s 773ms/step - loss: 0.0227 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.0353 - val_sparse_categorical_accuracy: 0.9720\n",
            "Epoch 27/45\n",
            "79/79 [==============================] - 61s 774ms/step - loss: 0.0218 - sparse_categorical_accuracy: 0.9813 - val_loss: 0.0393 - val_sparse_categorical_accuracy: 0.9700\n",
            "Epoch 28/45\n",
            "79/79 [==============================] - 63s 796ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9816 - val_loss: 0.0362 - val_sparse_categorical_accuracy: 0.9721\n",
            "Epoch 29/45\n",
            "79/79 [==============================] - 63s 792ms/step - loss: 0.0210 - sparse_categorical_accuracy: 0.9821 - val_loss: 0.0356 - val_sparse_categorical_accuracy: 0.9720\n",
            "Epoch 30/45\n",
            "79/79 [==============================] - 63s 794ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9824 - val_loss: 0.0357 - val_sparse_categorical_accuracy: 0.9722\n",
            "Epoch 31/45\n",
            "79/79 [==============================] - 66s 835ms/step - loss: 0.0200 - sparse_categorical_accuracy: 0.9829 - val_loss: 0.0341 - val_sparse_categorical_accuracy: 0.9729\n",
            "Epoch 32/45\n",
            "79/79 [==============================] - 67s 847ms/step - loss: 0.0190 - sparse_categorical_accuracy: 0.9834 - val_loss: 0.0361 - val_sparse_categorical_accuracy: 0.9723\n",
            "Epoch 33/45\n",
            "79/79 [==============================] - 68s 858ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9836 - val_loss: 0.0371 - val_sparse_categorical_accuracy: 0.9719\n",
            "Epoch 34/45\n",
            "79/79 [==============================] - 67s 844ms/step - loss: 0.0184 - sparse_categorical_accuracy: 0.9841 - val_loss: 0.0360 - val_sparse_categorical_accuracy: 0.9726\n",
            "Epoch 35/45\n",
            "79/79 [==============================] - 65s 828ms/step - loss: 0.0181 - sparse_categorical_accuracy: 0.9844 - val_loss: 0.0359 - val_sparse_categorical_accuracy: 0.9717\n",
            "Epoch 36/45\n",
            "79/79 [==============================] - 67s 854ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9852 - val_loss: 0.0376 - val_sparse_categorical_accuracy: 0.9724\n",
            "Epoch 37/45\n",
            "79/79 [==============================] - 71s 899ms/step - loss: 0.0173 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.0373 - val_sparse_categorical_accuracy: 0.9718\n",
            "Epoch 38/45\n",
            "79/79 [==============================] - 65s 821ms/step - loss: 0.0166 - sparse_categorical_accuracy: 0.9856 - val_loss: 0.0356 - val_sparse_categorical_accuracy: 0.9724\n",
            "Epoch 39/45\n",
            "79/79 [==============================] - 68s 861ms/step - loss: 0.0159 - sparse_categorical_accuracy: 0.9859 - val_loss: 0.0384 - val_sparse_categorical_accuracy: 0.9729\n",
            "Epoch 40/45\n",
            "79/79 [==============================] - 67s 852ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.0373 - val_sparse_categorical_accuracy: 0.9718\n",
            "Epoch 41/45\n",
            "79/79 [==============================] - 69s 874ms/step - loss: 0.0148 - sparse_categorical_accuracy: 0.9868 - val_loss: 0.0395 - val_sparse_categorical_accuracy: 0.9713\n",
            "Epoch 42/45\n",
            "79/79 [==============================] - 61s 768ms/step - loss: 0.0161 - sparse_categorical_accuracy: 0.9859 - val_loss: 0.0398 - val_sparse_categorical_accuracy: 0.9702\n",
            "Epoch 43/45\n",
            "79/79 [==============================] - 62s 780ms/step - loss: 0.0145 - sparse_categorical_accuracy: 0.9871 - val_loss: 0.0387 - val_sparse_categorical_accuracy: 0.9712\n",
            "Epoch 44/45\n",
            "79/79 [==============================] - 77s 970ms/step - loss: 0.0140 - sparse_categorical_accuracy: 0.9877 - val_loss: 0.0413 - val_sparse_categorical_accuracy: 0.9690\n",
            "Epoch 45/45\n",
            "79/79 [==============================] - 83s 1s/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.0396 - val_sparse_categorical_accuracy: 0.9719\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTLqqMMZhVar",
        "outputId": "729fe995-df32-4985-b504-7313d380b5df"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "preds = np.array([])\n",
        "y_trues= np.array([])\n",
        "\n",
        "# iterate through test set, make predictions based on trained model\n",
        "for input_example_batch, target_example_batch in ds_series_batch_test:\n",
        "\n",
        "  pred=model.predict_on_batch(input_example_batch)\n",
        "  pred_max=tf.argmax(tf.nn.softmax(pred),2).numpy().flatten()\n",
        "  y_true=target_example_batch.numpy().flatten()\n",
        "\n",
        "  preds=np.concatenate([preds,pred_max])\n",
        "  y_trues=np.concatenate([y_trues,y_true])\n",
        "\n",
        "# remove padding from evaluation\n",
        "remove_padding = [(p,y) for p,y in zip(preds,y_trues) if y!=0]\n",
        "\n",
        "r_p = [x[0] for x in remove_padding]\n",
        "r_t = [x[1] for x in remove_padding]\n",
        "\n",
        "# print confusion matrix and classification report\n",
        "print(confusion_matrix(r_p,r_t))\n",
        "print(classification_report(r_p,r_t, zero_division=0))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1948     0     6     0     0   107     0     0]\n",
            " [    2  1830    10     6    41   173     0     0]\n",
            " [    1     1  2210     3    55    99     0     0]\n",
            " [    6     8    25  1443     0   164     0     0]\n",
            " [    1    25    26     4  2753   152     5     0]\n",
            " [  118   427    72    53   217 68575   270    23]\n",
            " [    0     0     0     0     0   162   878    11]\n",
            " [    0     0     0     0     0    22    14   267]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.94      0.95      0.94      2061\n",
            "         2.0       0.80      0.89      0.84      2062\n",
            "         3.0       0.94      0.93      0.94      2369\n",
            "         4.0       0.96      0.88      0.91      1646\n",
            "         5.0       0.90      0.93      0.91      2966\n",
            "         6.0       0.99      0.98      0.99     69755\n",
            "         7.0       0.75      0.84      0.79      1051\n",
            "         8.0       0.89      0.88      0.88       303\n",
            "\n",
            "    accuracy                           0.97     82213\n",
            "   macro avg       0.89      0.91      0.90     82213\n",
            "weighted avg       0.97      0.97      0.97     82213\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "n05bUJ22hudl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f47f180-0378-4083-bcca-91cd36da17eb"
      }
    }
  ]
}