{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projeto-extract.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxg5F6gnsUQ0",
        "outputId": "5a578891-982d-4812-fd20-4fb3959db9cf"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "import ssl\n",
        "import json\n",
        "\n",
        "ctx = ssl.create_default_context()\n",
        "ctx.check_hostname = False\n",
        "ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "dataTrain = urlopen(\"https://raw.githubusercontent.com/budzianowski/multiwoz/master/data/MultiWOZ_2.2/train/dialogues_001.json\", context=ctx).read().decode()\n",
        "dataTest = urlopen(\"https://raw.githubusercontent.com/budzianowski/multiwoz/master/data/MultiWOZ_2.2/test/dialogues_001.json\", context=ctx).read().decode()\n",
        "\n",
        "myJsonTrain = json.loads(dataTrain)\n",
        "myJsonTest = json.loads(dataTest)\n",
        "print (json.dumps(myJsonTrain[0], indent=1)[0:100])\n",
        "print (json.dumps(myJsonTest[0], indent=1)[0:100])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            " \"dialogue_id\": \"PMUL4398.json\",\n",
            " \"services\": [\n",
            "  \"restaurant\",\n",
            "  \"hotel\"\n",
            " ],\n",
            " \"turns\": [\n",
            "  {\n",
            "   \"\n",
            "{\n",
            " \"dialogue_id\": \"MUL0484.json\",\n",
            " \"services\": [\n",
            "  \"attraction\",\n",
            "  \"train\"\n",
            " ],\n",
            " \"turns\": [\n",
            "  {\n",
            "   \"f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcXfKcxq6h3s",
        "outputId": "c93eab61-f3c9-4344-d683-44e62d1488a7"
      },
      "source": [
        "#http://alexminnaar.com/2019/08/22/ner-rnns-tensorflow.html\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuAhXdtyApXj"
      },
      "source": [
        "import re\n",
        "\n",
        "from nltk.util import ngrams\n",
        "\n",
        "timepat = re.compile(\"\\d{1,2}[:]\\d{1,2}\")\n",
        "pricepat = re.compile(\"\\d{1,3}[.]\\d{1,2}\")\n",
        "\n",
        "fin = urlopen(\"https://raw.githubusercontent.com/budzianowski/multiwoz/master/utils/mapping.pair\", context=ctx)\n",
        "replacements = []\n",
        "for line in fin.readlines():\n",
        "    line = line.decode().replace('\\n', '')\n",
        "    tok_from, tok_to = line.replace('\\n', '').split('\\t')\n",
        "    replacements.append((' ' + tok_from + ' ', ' ' + tok_to + ' '))\n",
        "\n",
        "\n",
        "def insertSpace(token, text):\n",
        "    sidx = 0\n",
        "    while True:\n",
        "        sidx = text.find(token, sidx)\n",
        "        if sidx == -1:\n",
        "            break\n",
        "        if sidx + 1 < len(text) and re.match('[0-9]', text[sidx - 1]) and \\\n",
        "                re.match('[0-9]', text[sidx + 1]):\n",
        "            sidx += 1\n",
        "            continue\n",
        "        if text[sidx - 1] != ' ':\n",
        "            text = text[:sidx] + ' ' + text[sidx:]\n",
        "            sidx += 1\n",
        "        if sidx + len(token) < len(text) and text[sidx + len(token)] != ' ':\n",
        "            text = text[:sidx + 1] + ' ' + text[sidx + 1:]\n",
        "        sidx += 1\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalize(text):\n",
        "    # lower case every word\n",
        "    text = text.lower()\n",
        "\n",
        "    # replace white spaces in front and end\n",
        "    text = re.sub(r'^\\s*|\\s*$', '', text)\n",
        "\n",
        "    # hotel domain pfb30\n",
        "    text = re.sub(r\"b&b\", \"bed and breakfast\", text)\n",
        "    text = re.sub(r\"b and b\", \"bed and breakfast\", text)\n",
        "\n",
        "    # normalize phone number\n",
        "    ms = re.findall('\\(?(\\d{3})\\)?[-.\\s]?(\\d{3})[-.\\s]?(\\d{4,5})', text)\n",
        "    if ms:\n",
        "        sidx = 0\n",
        "        for m in ms:\n",
        "            sidx = text.find(m[0], sidx)\n",
        "            if text[sidx - 1] == '(':\n",
        "                sidx -= 1\n",
        "            eidx = text.find(m[-1], sidx) + len(m[-1])\n",
        "            text = text.replace(text[sidx:eidx], ''.join(m))\n",
        "\n",
        "    # normalize postcode\n",
        "    ms = re.findall('([a-z]{1}[\\. ]?[a-z]{1}[\\. ]?\\d{1,2}[, ]+\\d{1}[\\. ]?[a-z]{1}[\\. ]?[a-z]{1}|[a-z]{2}\\d{2}[a-z]{2})',\n",
        "                    text)\n",
        "    if ms:\n",
        "        sidx = 0\n",
        "        for m in ms:\n",
        "            sidx = text.find(m, sidx)\n",
        "            eidx = sidx + len(m)\n",
        "            text = text[:sidx] + re.sub('[,\\. ]', '', m) + text[eidx:]\n",
        "\n",
        "    # weird unicode bug\n",
        "    text = re.sub(u\"(\\u2018|\\u2019)\", \"'\", text)\n",
        "\n",
        "    # replace time and and price\n",
        "    times = re.findall(timepat, text)\n",
        "    prices = re.findall(pricepat, text)\n",
        "    text = re.sub(timepat, ' [value_time] ', text)\n",
        "    text = re.sub(pricepat, ' [value_price] ', text)\n",
        "    #text = re.sub(pricepat2, '[value_price]', text)\n",
        "\n",
        "    # replace st.\n",
        "    text = text.replace(';', ',')\n",
        "    text = re.sub('$\\/', '', text)\n",
        "    text = text.replace('/', ' and ')\n",
        "\n",
        "    # replace other special characters\n",
        "    text = text.replace('-', ' ')\n",
        "    text = re.sub('[\\\":\\<>@\\(\\)]', '', text)\n",
        "\n",
        "    # insert white space before and after tokens:\n",
        "    for token in ['?', '.', ',', '!']:\n",
        "        text = insertSpace(token, text)\n",
        "\n",
        "    # insert white space for 's\n",
        "    text = insertSpace('\\'s', text)\n",
        "\n",
        "    # replace it's, does't, you'd ... etc\n",
        "    text = re.sub('^\\'', '', text)\n",
        "    text = re.sub('\\'$', '', text)\n",
        "    text = re.sub('\\'\\s', ' ', text)\n",
        "    text = re.sub('\\s\\'', ' ', text)\n",
        "    for fromx, tox in replacements:\n",
        "        text = ' ' + text + ' '\n",
        "        text = text.replace(fromx, tox)[1:-1]\n",
        "\n",
        "    # remove multiple spaces\n",
        "    text = re.sub(' +', ' ', text)\n",
        "\n",
        "    # concatenate numbers\n",
        "    tmp = text\n",
        "    tokens = text.split()\n",
        "    i = 1\n",
        "    while i < len(tokens):\n",
        "        if re.match(u'^\\d+$', tokens[i]) and \\\n",
        "                re.match(u'\\d+$', tokens[i - 1]):\n",
        "            tokens[i - 1] += tokens[i]\n",
        "            del tokens[i]\n",
        "        else:\n",
        "            i += 1\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    for time in times:\n",
        "      text = re.sub('\\\\[value_time\\\\]', time, text, 1)\n",
        "\n",
        "    for price in prices:\n",
        "      text = re.sub('\\\\[value_price\\\\]', price, text, 1)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zXUsM586k3E",
        "outputId": "39ebed2b-dd8a-4453-c6bf-4bc8d367067e"
      },
      "source": [
        "# filter\n",
        "import string\n",
        "labels = set()\n",
        "\n",
        "def filter(data):\n",
        "  global labels\n",
        "\n",
        "  result = []\n",
        "\n",
        "  for i, obj in enumerate(data):\n",
        "      turns = obj.get(\"turns\", None)\n",
        "      for turn in turns:\n",
        "          if turn:\n",
        "              utterance = turn.get(\"utterance\", None)\n",
        "              if utterance:\n",
        "                  frames = turn.get(\"frames\", None)\n",
        "                  if frames:\n",
        "                      dic = dict()\n",
        "                      res = [[], []]\n",
        "                      for frame in frames: \n",
        "                          state = frame.get(\"state\", None)\n",
        "                          service = frame.get(\"service\", None)\n",
        "                          if service and service == \"restaurant\" and state:\n",
        "                              slot_values = state.get(\"slot_values\", None)\n",
        "                              if slot_values and len(slot_values) != 0:\n",
        "                                  for key, value in slot_values.items():\n",
        "                                      for elem in value:\n",
        "                                        dic[elem] = key\n",
        "\n",
        "                      text = normalize(utterance)\n",
        "                      for word in text.split(\" \"):\n",
        "                          res[0].append(word)\n",
        "                          res[1].append(dic.get(word, \"O\"))\n",
        "                          labels.add(dic.get(word, \"O\"))    \n",
        "                      \n",
        "                      result.append(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return result\n",
        "\n",
        "train_dataset = filter(myJsonTrain)\n",
        "for i, elem in enumerate(train_dataset):\n",
        "      # print(elem)\n",
        "      if i > 30:\n",
        "        # print(\"--------------------\")\n",
        "        break\n",
        "test_dataset = filter(myJsonTest)\n",
        "for i, elem in enumerate(test_dataset):\n",
        "      # print(elem)\n",
        "      if i > 30:\n",
        "        # print(\"--------------------\")\n",
        "        break\n",
        "\n",
        "print(labels)\n",
        "test_dataset[6:9]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'restaurant-food', 'restaurant-booktime', 'O', 'restaurant-area', 'restaurant-bookpeople', 'restaurant-bookday', 'restaurant-name', 'restaurant-pricerange'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['can',\n",
              "   'you',\n",
              "   'book',\n",
              "   'me',\n",
              "   'a',\n",
              "   'table',\n",
              "   'for',\n",
              "   '11:00',\n",
              "   'on',\n",
              "   'friday',\n",
              "   '?'],\n",
              "  ['O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'O',\n",
              "   'restaurant-booktime',\n",
              "   'O',\n",
              "   'restaurant-bookday',\n",
              "   'O']],\n",
              " [['actually', ',', 'for', '4', ',', 'please', '.'],\n",
              "  ['O', 'O', 'O', 'restaurant-bookpeople', 'O', 'O', 'O']],\n",
              " [['great',\n",
              "   ',',\n",
              "   'can',\n",
              "   'you',\n",
              "   'also',\n",
              "   'get',\n",
              "   'me',\n",
              "   'information',\n",
              "   'or',\n",
              "   'architecture',\n",
              "   'in',\n",
              "   'the',\n",
              "   'area'],\n",
              "  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ank-S_QRTpYU",
        "outputId": "1a9e6b9a-2b2f-461d-e5b3-cad192f83e18"
      },
      "source": [
        "for i, label in enumerate(train_dataset[1]):\n",
        "    if 'other' in label:\n",
        "      train_dataset[1][i] = 'O'\n",
        "\n",
        "for i, label in enumerate(test_dataset[1]):\n",
        "    if 'other' in label:\n",
        "      test_dataset[1][i] = 'O'\n",
        "      \n",
        "if 'other' in labels: labels.remove('other')\n",
        "labels.add('O')\n",
        "print(labels)\n",
        "\n",
        "# create character vocab\n",
        "all_text = \" \".join([\" \".join(x[0]) for x in train_dataset+test_dataset])\n",
        "vocab = sorted(set(all_text))\n",
        "\n",
        "# create character/id and label/id mapping\n",
        "char2idx = {u:i+1 for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "label2idx = {u:i+1 for i, u in enumerate(labels)}\n",
        "idx2label = np.array(labels)\n",
        "\n",
        "print(idx2label)\n",
        "print(char2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'restaurant-food', 'restaurant-booktime', 'O', 'restaurant-area', 'restaurant-bookpeople', 'restaurant-bookday', 'restaurant-name', 'restaurant-pricerange'}\n",
            "{'restaurant-food', 'restaurant-booktime', 'O', 'restaurant-area', 'restaurant-bookpeople', 'restaurant-bookday', 'restaurant-name', 'restaurant-pricerange'}\n",
            "{' ': 1, '!': 2, '#': 3, '$': 4, '&': 5, \"'\": 6, '*': 7, ',': 8, '-': 9, '.': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, '?': 22, '`': 23, 'a': 24, 'b': 25, 'c': 26, 'd': 27, 'e': 28, 'f': 29, 'g': 30, 'h': 31, 'i': 32, 'j': 33, 'k': 34, 'l': 35, 'm': 36, 'n': 37, 'o': 38, 'p': 39, 'q': 40, 'r': 41, 's': 42, 't': 43, 'u': 44, 'v': 45, 'w': 46, 'x': 47, 'y': 48, 'z': 49}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU3whO-CXynY",
        "outputId": "896de91f-4c26-47d4-ffe1-997b59aafff3"
      },
      "source": [
        "def split_char_labels(eg):\n",
        "  '''\n",
        "  For a given input/output example, break tokens into characters while keeping \n",
        "  the same label.\n",
        "  '''\n",
        "\n",
        "  tokens = eg[0]\n",
        "  labels = eg[1]\n",
        "\n",
        "  input_chars = []\n",
        "  output_char_labels = []\n",
        "\n",
        "  for token,label in zip(tokens,labels):\n",
        "    input_chars.extend([char for char in token])\n",
        "    input_chars.extend(' ')\n",
        "    output_char_labels.extend([label]*len(token))\n",
        "    output_char_labels.extend('O')\n",
        "\n",
        "  return [[char2idx[x] for x in input_chars[:-1]], np.array([label2idx[x] for x in output_char_labels[:-1]])]\n",
        "\n",
        "\n",
        "train_formatted = [split_char_labels(eg) for eg in train_dataset]\n",
        "test_formatted = [split_char_labels(eg) for eg in test_dataset]\n",
        "\n",
        "print(len(train_formatted))\n",
        "print(len(test_formatted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3907\n",
            "4242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_JyTYupffwU",
        "outputId": "ceeaeb3a-1dd5-4dd4-b38b-cc784794efcf"
      },
      "source": [
        "# training generator\n",
        "def gen_train_series():\n",
        "\n",
        "    for eg in train_formatted:\n",
        "      yield eg[0],eg[1]\n",
        "\n",
        "# validation generator\n",
        "def gen_valid_series():\n",
        "\n",
        "    for eg in valid_formatted:\n",
        "      yield eg[0],eg[1]\n",
        "\n",
        "# test generator\n",
        "def gen_test_series():\n",
        "\n",
        "  for eg in test_formatted:\n",
        "      yield eg[0],eg[1]\n",
        "  \n",
        "# create Dataset objects for train, test and validation sets  \n",
        "series = tf.data.Dataset.from_generator(gen_train_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
        "series_test = tf.data.Dataset.from_generator(gen_test_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE=1000\n",
        "\n",
        "# create padded batch series objects for train, test and validation sets\n",
        "ds_series_batch = series.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
        "ds_series_batch_test = series_test.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
        "\n",
        "# print example batches\n",
        "for input_example_batch, target_example_batch in ds_series_batch_test.take(1):\n",
        "  print(input_example_batch)\n",
        "  print(target_example_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[32  1 37 ...  0  0  0]\n",
            " [32  1 46 ...  0  0  0]\n",
            " [25 28 29 ...  0  0  0]\n",
            " ...\n",
            " [26 24 37 ...  0  0  0]\n",
            " [43 31 24 ...  0  0  0]\n",
            " [32  1 46 ...  0  0  0]], shape=(128, 154), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[3 3 3 ... 0 0 0]\n",
            " [3 3 3 ... 0 0 0]\n",
            " [3 3 3 ... 0 0 0]\n",
            " ...\n",
            " [3 3 3 ... 0 0 0]\n",
            " [3 3 3 ... 0 0 0]\n",
            " [3 3 3 ... 0 0 0]], shape=(128, 154), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpnnY0sago83",
        "outputId": "f0f00758-3822-4d43-c2d0-0a382eae777c"
      },
      "source": [
        "vocab_size = len(vocab)+1\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "label_size = len(labels)  \n",
        "\n",
        "# build LSTM model\n",
        "def build_model(vocab_size,label_size, embedding_dim, rnn_units, batch_size):\n",
        "      model = tf.keras.Sequential([\n",
        "          tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                    batch_input_shape=[batch_size, None],mask_zero=True),\n",
        "          tf.keras.layers.LSTM(rnn_units,\n",
        "                               return_sequences=True,\n",
        "                               stateful=True,\n",
        "                               recurrent_initializer='glorot_uniform'),\n",
        "          tf.keras.layers.Dense(label_size)\n",
        "          ])\n",
        "      return model\n",
        "\n",
        "model = build_model(\n",
        "        vocab_size=len(vocab)+1,\n",
        "        label_size=len(labels)+1,\n",
        "        embedding_dim=embedding_dim,\n",
        "        rnn_units=rnn_units,\n",
        "        batch_size=BATCH_SIZE)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (128, None, 256)          12800     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (128, None, 1024)         5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 9)            9225      \n",
            "=================================================================\n",
            "Total params: 5,269,001\n",
            "Trainable params: 5,269,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTP-FbDbgyPz"
      },
      "source": [
        "import os\n",
        "\n",
        "# define loss function\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss,metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTLqqMMZhVar",
        "outputId": "1137bd83-c628-4162-acea-642efd3740bf"
      },
      "source": [
        "EPOCHS=20\n",
        "history = model.fit(ds_series_batch, epochs=EPOCHS, validation_data=ds_series_batch_test,callbacks=[checkpoint_callback])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "30/30 [==============================] - 774s 26s/step - loss: 0.1277 - sparse_categorical_accuracy: 0.9439 - val_loss: 0.0588 - val_sparse_categorical_accuracy: 0.9772\n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 750s 25s/step - loss: 0.0552 - sparse_categorical_accuracy: 0.9775 - val_loss: 0.0565 - val_sparse_categorical_accuracy: 0.9772\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 747s 25s/step - loss: 0.0524 - sparse_categorical_accuracy: 0.9776 - val_loss: 0.0535 - val_sparse_categorical_accuracy: 0.9772\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 735s 25s/step - loss: 0.0514 - sparse_categorical_accuracy: 0.9776 - val_loss: 0.0515 - val_sparse_categorical_accuracy: 0.9772\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 752s 25s/step - loss: 0.0491 - sparse_categorical_accuracy: 0.9773 - val_loss: 0.0498 - val_sparse_categorical_accuracy: 0.9772\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 744s 25s/step - loss: 0.0483 - sparse_categorical_accuracy: 0.9771 - val_loss: 0.0511 - val_sparse_categorical_accuracy: 0.9752\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 742s 25s/step - loss: 0.0481 - sparse_categorical_accuracy: 0.9772 - val_loss: 0.0497 - val_sparse_categorical_accuracy: 0.9772\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 757s 25s/step - loss: 0.0457 - sparse_categorical_accuracy: 0.9775 - val_loss: 0.0464 - val_sparse_categorical_accuracy: 0.9772\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 752s 25s/step - loss: 0.0457 - sparse_categorical_accuracy: 0.9769 - val_loss: 0.0448 - val_sparse_categorical_accuracy: 0.9772\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 752s 25s/step - loss: 0.0432 - sparse_categorical_accuracy: 0.9772 - val_loss: 0.0427 - val_sparse_categorical_accuracy: 0.9772\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 767s 26s/step - loss: 0.0412 - sparse_categorical_accuracy: 0.9777 - val_loss: 0.0419 - val_sparse_categorical_accuracy: 0.9772\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 764s 26s/step - loss: 0.0398 - sparse_categorical_accuracy: 0.9773 - val_loss: 0.0407 - val_sparse_categorical_accuracy: 0.9775\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 756s 25s/step - loss: 0.0391 - sparse_categorical_accuracy: 0.9776 - val_loss: 0.0417 - val_sparse_categorical_accuracy: 0.9773\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 762s 26s/step - loss: 0.0370 - sparse_categorical_accuracy: 0.9777 - val_loss: 0.0394 - val_sparse_categorical_accuracy: 0.9777\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 757s 25s/step - loss: 0.0384 - sparse_categorical_accuracy: 0.9772 - val_loss: 0.0392 - val_sparse_categorical_accuracy: 0.9776\n",
            "Epoch 16/20\n",
            "30/30 [==============================] - 756s 25s/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9777 - val_loss: 0.0350 - val_sparse_categorical_accuracy: 0.9779\n",
            "Epoch 17/20\n",
            "30/30 [==============================] - 750s 25s/step - loss: 0.0344 - sparse_categorical_accuracy: 0.9777 - val_loss: 0.0358 - val_sparse_categorical_accuracy: 0.9777\n",
            "Epoch 18/20\n",
            "30/30 [==============================] - 759s 26s/step - loss: 0.0339 - sparse_categorical_accuracy: 0.9778 - val_loss: 0.0344 - val_sparse_categorical_accuracy: 0.9778\n",
            "Epoch 19/20\n",
            "30/30 [==============================] - 755s 25s/step - loss: 0.0321 - sparse_categorical_accuracy: 0.9782 - val_loss: 0.0331 - val_sparse_categorical_accuracy: 0.9780\n",
            "Epoch 20/20\n",
            "30/30 [==============================] - 749s 25s/step - loss: 0.0313 - sparse_categorical_accuracy: 0.9785 - val_loss: 0.0316 - val_sparse_categorical_accuracy: 0.9775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n05bUJ22hudl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d791de-02e7-42b3-e559-e9e0063e6b80"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "preds = np.array([])\n",
        "y_trues= np.array([])\n",
        "\n",
        "# iterate through test set, make predictions based on trained model\n",
        "for input_example_batch, target_example_batch in ds_series_batch_test:\n",
        "\n",
        "  pred=model.predict_on_batch(input_example_batch)\n",
        "  pred_max=tf.argmax(tf.nn.softmax(pred),2).numpy().flatten()\n",
        "  y_true=target_example_batch.numpy().flatten()\n",
        "\n",
        "  preds=np.concatenate([preds,pred_max])\n",
        "  y_trues=np.concatenate([y_trues,y_true])\n",
        "\n",
        "# remove padding from evaluation\n",
        "remove_padding = [(p,y) for p,y in zip(preds,y_trues) if y!=0]\n",
        "\n",
        "r_p = [x[0] for x in remove_padding]\n",
        "r_t = [x[1] for x in remove_padding]\n",
        "\n",
        "# print confusion matrix and classification report\n",
        "print(confusion_matrix(r_p,r_t))\n",
        "print(classification_report(r_p,r_t))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[     0      0      0      0      0      0      0      0]\n",
            " [     0      6      0      0      0      0      0      0]\n",
            " [  1571    900 268085    840    181   1137    187   1329]\n",
            " [     0      0      0      0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0]\n",
            " [     0      0     17      0      0     78      0      0]\n",
            " [     0      0      0      0      0      0      0      0]\n",
            " [     1      0     21      0      0      0      0     17]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.00      0.00      0.00         0\n",
            "         2.0       0.01      1.00      0.01         6\n",
            "         3.0       1.00      0.98      0.99    274230\n",
            "         4.0       0.00      0.00      0.00         0\n",
            "         5.0       0.00      0.00      0.00         0\n",
            "         6.0       0.06      0.82      0.12        95\n",
            "         7.0       0.00      0.00      0.00         0\n",
            "         8.0       0.01      0.44      0.02        39\n",
            "\n",
            "    accuracy                           0.98    274370\n",
            "   macro avg       0.14      0.40      0.14    274370\n",
            "weighted avg       1.00      0.98      0.99    274370\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
